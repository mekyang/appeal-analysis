import pandas as pd
import numpy as np
import umap
import hdbscan
import jieba
import pickle
import os
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Optional, Union

class TaxClusteringEngine:
    """
    ç¨åŠ¡æ–‡æœ¬èšç±»åˆ†æå¼•æ“ (Tax Clustering Engine)
    
    åŸºäº Sentence-BERT + UMAP + HDBSCAN çš„ç°ä»£æ–‡æœ¬æŒ–æ˜æµç¨‹ã€‚
    ç”¨äºå°†éç»“æ„åŒ–çš„ç¨åŠ¡å’¨è¯¢æ–‡æœ¬è½¬åŒ–ä¸ºç»“æ„åŒ–çš„èšç±»è¯é¢˜ã€‚
    """

    def __init__(self, model_name: str = None):
        """
        [Public] åˆå§‹åŒ–åˆ†æå¼•æ“ï¼Œä¼˜å…ˆä»æœ¬åœ°ç›®å½•åŠ è½½ SBERT æ¨¡å‹ã€‚

        Args:
            model_name (str): æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ– HuggingFace åç§°ã€‚è‹¥ä¸º Noneï¼Œåˆ™é»˜è®¤ä½¿ç”¨
                              å·¥ä½œåŒºä¸‹çš„æœ¬åœ°æ¨¡å‹ `models/BAAI/bge-large-zh-v1.5`ã€‚
        """
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ç›®å½•ï¼Œé™ä½åœ¨çº¿åŠ è½½æˆ– repo id æ ¡éªŒå¤±è´¥çš„é£é™©
        default_local = os.path.join('models', 'BAAI', 'bge-large-zh-v1.5')

        # å¦‚æœç”¨æˆ·æŒ‡å®šäº† model_nameï¼Œæ£€æŸ¥å®ƒæ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
        if model_name:
            if os.path.exists(model_name):
                chosen_model = model_name
            else:
                # å°è¯•åœ¨ models/ ç›®å½•ä¸‹æŸ¥æ‰¾
                candidate = os.path.join('models', model_name)
                if os.path.exists(candidate):
                    chosen_model = candidate
                else:
                    print(f"âš ï¸ æä¾›çš„æ¨¡å‹è·¯å¾„ '{model_name}' æœªæ‰¾åˆ°ï¼Œæœ¬åœ°å›é€€åˆ° {default_local}")
                    chosen_model = default_local
        else:
            chosen_model = default_local

        print(f"ğŸ¤– [Init] æ­£åœ¨åŠ è½½æ¨¡å‹: {chosen_model}...")
        try:
            self.model = SentenceTransformer(chosen_model)
        except Exception as e:
            print(f"âŒ æœ¬åœ°åŠ è½½å¤±è´¥ ({e})ï¼Œå°è¯•ä½¿ç”¨åœ¨çº¿æ¨¡å‹ paraphrase-multilingual-MiniLM-L12-v2 åŠ è½½...")
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # --- å†…éƒ¨çŠ¶æ€å˜é‡ (State) ---
        self._embeddings = None       # SBERT åŸå§‹å‘é‡
        self._umap_embeddings = None  # é™ç»´åå‘é‡
        self._labels = None           # èšç±»æ ‡ç­¾
        self.results_df = None        # æœ€ç»ˆç»“æœè¡¨
        
    def run_analysis(self, 
                     texts: List[str],
                     original: Optional[List[Union[int, str]]] = None, 
                     n_neighbors: int = 15, 
                     n_components: int = 5, 
                     min_cluster_size: int = 10,
                     keyword_top_n: int = 5,
                     progress_callback=None) -> pd.DataFrame:
        """
        [Public] æ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹ï¼šç¼–ç  -> é™ç»´ -> èšç±» -> å…³é”®è¯æå–ã€‚

        Args:
            texts (List[str]): å·²ç»é¢„å¤„ç†å¥½çš„æ–‡æœ¬åˆ—è¡¨ï¼ˆList of stringsï¼‰ã€‚
            original (List[Union[int, str]]): åŸå§‹æ•°æ®çš„ ID åˆ—è¡¨ï¼ˆå¦‚ Trace_IDï¼‰ã€‚
            n_neighbors (int): UMAP å‚æ•°ã€‚æ§åˆ¶å±€éƒ¨ç»“æ„ï¼Œè¶Šå°è¶Šå…³æ³¨ç»†èŠ‚ã€‚å»ºè®® 10-30ã€‚
            n_components (int): UMAP å‚æ•°ã€‚é™ç»´åçš„ç»´åº¦æ•°ï¼ŒHDBSCAN æ¨è 5-10ã€‚
            min_cluster_size (int): HDBSCAN å‚æ•°ã€‚æœ€å°æˆå›¢æ•°é‡ï¼Œå°äºæ­¤æ•°é‡è§†ä¸ºå™ªéŸ³ã€‚
            keyword_top_n (int): æ¯ä¸ªèšç±»æå–å¤šå°‘ä¸ªæ ¸å¿ƒå…³é”®è¯ã€‚
            progress_callback (callable): å¯é€‰çš„è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç­¾åä¸º progress_callback(current, total, stage)

        Returns:
            pd.DataFrame: åŒ…å«åŸå§‹æ–‡æœ¬ã€èšç±»IDã€æ ¸å¿ƒå…³é”®è¯çš„å®Œæ•´æ•°æ®æ¡†ã€‚
        """
        if not texts:
            raise ValueError("è¾“å…¥æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©ºï¼")
        
        if original is not None and len(texts) != len(original):
            raise ValueError(f"æ–‡æœ¬æ•°é‡ ({len(texts)}) ä¸ åŸå§‹æ•°æ®ID æ•°é‡ ({len(original)}) ä¸ä¸€è‡´ï¼")

        print(f"ğŸš€ å¼€å§‹åˆ†æ {len(texts)} æ¡æ•°æ®...")

        # 1. å‘é‡åŒ–
        self._encode_text(texts, progress_callback)

        # 2. é™ç»´
        if progress_callback:
            progress_callback(35, 100, "ğŸ“‰ UMAPé™ç»´ä¸­...")
        self._reduce_dimensions(n_neighbors, n_components)

        # 3. èšç±»
        if progress_callback:
            progress_callback(60, 100, "ğŸ” HDBSCANèšç±»ä¸­...")
        self._perform_clustering(min_cluster_size)

        # 4. ç»“æœæ•´åˆä¸å…³é”®è¯æå–
        if progress_callback:
            progress_callback(85, 100, "ğŸ·ï¸ æå–å…³é”®è¯ä¸­...")
        self._generate_final_report(texts, keyword_top_n, original)

        if progress_callback:
            progress_callback(100, 100, "âœ… åˆ†æå®Œæˆï¼")
        
        print("âœ… åˆ†ææµç¨‹ç»“æŸã€‚")
        return self.results_df

    def _encode_text(self, texts: List[str], progress_callback=None) -> None:
        """
        [Private] ä½¿ç”¨ SBERT å°†æ–‡æœ¬è½¬åŒ–ä¸ºé«˜ç»´è¯­ä¹‰å‘é‡ã€‚
        ç»“æœå­˜å‚¨äº self._embeddings
        """
        print("ğŸ§  [Step 1] æ­£åœ¨è¿›è¡Œè¯­ä¹‰ç¼–ç  (SBERT)...")
        
        # SBERT ç¼–ç  - ä½¿ç”¨ show_progress_bar=False é¿å…ä¸å‰ç«¯è¿›åº¦æ¡å†²çª
        # æ‰‹åŠ¨æ¨¡æ‹Ÿè¿›åº¦
        batch_size = 32
        total_batches = (len(texts) + batch_size - 1) // batch_size
        embeddings_list = []
        
        for i, batch_start in enumerate(range(0, len(texts), batch_size)):
            batch_end = min(batch_start + batch_size, len(texts))
            batch = texts[batch_start:batch_end]
            
            batch_embeddings = self.model.encode(batch, show_progress_bar=False)
            embeddings_list.append(batch_embeddings)
            
            if progress_callback:
                current = batch_end
                progress = int(5 + (current / len(texts)) * 25)  # è¿›åº¦ 5% -> 30%
                progress_callback(progress, 100, f"ğŸ§  SBERTç¼–ç ä¸­ ({i+1}/{total_batches})...")
        
        self._embeddings = np.vstack(embeddings_list)
        
        if progress_callback:
            progress_callback(30, 100, "ğŸ§  SBERTç¼–ç å®Œæˆ...")

    def _reduce_dimensions(self, n_neighbors: int, n_components: int) -> None:
        """
        [Private] ä½¿ç”¨ UMAP å¯¹å‘é‡è¿›è¡Œé™ç»´ï¼Œä»¥ä¾¿äºå¯†åº¦èšç±»ã€‚
        ç»“æœå­˜å‚¨äº self._umap_embeddings
        """
        print(f"ğŸ“‰ [Step 2] æ­£åœ¨é™ç»´ UMAP (neighbors={n_neighbors}, dim={n_components})...")
        reducer = umap.UMAP(
            n_neighbors=n_neighbors,
            n_components=n_components,
            metric='cosine',  # æ–‡æœ¬æ•°æ®æ¨èä½¿ç”¨ä½™å¼¦è·ç¦»
            random_state=42   # å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
        )
        self._umap_embeddings = reducer.fit_transform(self._embeddings)

    def _perform_clustering(self, min_cluster_size: int) -> None:
        """
        [Private] ä½¿ç”¨ HDBSCAN è¿›è¡ŒåŸºäºå¯†åº¦çš„èšç±»ã€‚
        ç»“æœå­˜å‚¨äº self._labels
        """
        print(f"ğŸ” [Step 3] æ­£åœ¨èšç±» HDBSCAN (min_size={min_cluster_size})...")
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            metric='euclidean',        # åœ¨ä½ç»´ç©ºé—´ä½¿ç”¨æ¬§æ°è·ç¦»å³å¯
            cluster_selection_method='eom'
        )
        self._labels = clusterer.fit_predict(self._umap_embeddings)
        
        # ç®€å•ç»Ÿè®¡
        unique_labels = set(self._labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        noise_ratio = list(self._labels).count(-1) / len(self._labels)
        print(f"   -> å‘ç° {n_clusters} ä¸ªèšç±»ï¼Œå™ªéŸ³æ¯”ä¾‹: {noise_ratio:.2%}")

    def _extract_cluster_keywords(self, df: pd.DataFrame, top_n: int) -> Dict[int, str]:
        """
        [Private] åŸºäº c-TF-IDF æ€æƒ³æå–æ¯ä¸€ç±»çš„å…³é”®è¯ã€‚
        
        Args:
            df (pd.DataFrame): åŒ…å« 'Cluster' å’Œ 'Text' åˆ—çš„ä¸´æ—¶è¡¨ã€‚
            top_n (int): æå–å‰ N ä¸ªè¯ã€‚
            
        Returns:
            Dict[int, str]: {èšç±»ID: "å…³é”®è¯1, å…³é”®è¯2..."}
        """
        print("ğŸ·ï¸ [Step 4] æ­£åœ¨æå–æ ¸å¿ƒå…³é”®è¯...")
        
        # 1. æŒ‰èšç±»åˆå¹¶æ–‡æœ¬ (Document Aggregation)
        # å¿½ç•¥å™ªéŸ³æ•°æ® (-1)
        docs_df = df[df['Cluster'] != -1].copy()
        
        # 2. å†…ç½®åˆ†è¯å™¨ (é’ˆå¯¹å…³é”®è¯ç»Ÿè®¡ï¼Œéœ€è¦æŠŠå¥å­åˆ‡å¼€)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªç”¨jiebaåšç®€å•çš„åˆ‡è¯ï¼Œä¸éœ€è¦å¤ªå¤æ‚çš„æ¸…æ´—ï¼Œå› ä¸ºCountVectorizeræœ‰stop_words
        def tokenizer(text):
            return jieba.lcut(text)

        # 3. å‡†å¤‡åˆå¹¶åçš„æ–‡æœ¬
        # å°†åŒä¸€ç±»çš„æ‰€æœ‰æ–‡æœ¬æ‹¼æˆä¸€ä¸ªè¶…é•¿å­—ç¬¦ä¸²
        docs_per_class = docs_df.groupby(['Cluster'], as_index=False).agg({'Text': ' '.join})
        
        # 4. ä½¿ç”¨ CountVectorizer è®¡ç®—ç±»å†…è¯é¢‘
        # æ”¹ç”¨ç»å¯¹æ•°å€¼é¿å…å°æ•°æ®é›†æ—¶çš„å‚æ•°å†²çª
        n_docs = len(docs_per_class)
        # åŠ¨æ€è®¡ç®—å‚æ•°ï¼šmin_df æœ€å¤šä¸ºæ–‡æ¡£æ€»æ•°çš„ 50%ï¼Œmax_df ç•™åœ¨ 70% ä»¥ä¸Š
        min_df_value = max(1, n_docs // 2) if n_docs > 2 else 1
        max_df_value = max(n_docs - 1, int(n_docs * 0.7))
        
        count_vectorizer = CountVectorizer(
            tokenizer=tokenizer,
            stop_words=['çš„', 'äº†', 'æˆ‘', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ'], # åŸºç¡€åœç”¨è¯
            max_df=max_df_value,  # ä½¿ç”¨ç»å¯¹å€¼ï¼šè‡³å°‘å‡ºç°åœ¨ 70% çš„ç°‡
            min_df=min_df_value   # ä½¿ç”¨ç»å¯¹å€¼ï¼šè‡³å°‘å‡ºç°åœ¨ 1-2 ä¸ªç°‡
        )
        
        count_matrix = count_vectorizer.fit_transform(docs_per_class['Text'])
        words = count_vectorizer.get_feature_names_out()
        
        # 5. æå– Top N
        keywords_dict = {}
        # count_matrix æ˜¯ (n_clusters, n_words) çš„çŸ©é˜µ
        matrix_array = count_matrix.toarray()
        
        for i, row in docs_per_class.iterrows():
            cluster_id = row['Cluster']
            # è·å–è¯¥è¡Œçš„è¯é¢‘å‘é‡
            word_counts = matrix_array[i]
            # è·å–æ’åºåçš„ç´¢å¼• (ä»å¤§åˆ°å°)
            sorted_indices = word_counts.argsort()[::-1]
            
            top_keywords = []
            for idx in sorted_indices[:top_n]:
                top_keywords.append(words[idx])
            
            keywords_dict[cluster_id] = ", ".join(top_keywords)
            
        return keywords_dict

    def _generate_final_report(self, texts: List[str], keyword_top_n: int, original: Optional[List] = None) -> None:
        """
        [Private] ç»„è£…æœ€ç»ˆçš„ DataFrameã€‚
        ç»“æœå­˜å‚¨äº self.results_df
        """
        # åˆ›å»ºä¸´æ—¶ DataFrame
        data = {
            'Text': texts, 
            'Cluster': self._labels
        }
        
        # å¦‚æœä¼ å…¥äº† IDï¼Œåˆ™æ·»åŠ åˆ°å­—å…¸ä¸­
        if original is not None:
            data['Trace_ID'] = original
            
        # åˆ›å»º DataFrame
        df = pd.DataFrame(data)
        
        # æå–å…³é”®è¯å­—å…¸
        keywords_map = self._extract_cluster_keywords(df, keyword_top_n)
        
        # æ˜ å°„å›ä¸»è¡¨
        df['Keywords'] = df['Cluster'].map(keywords_map)
        df['Keywords'] = df['Keywords'].fillna("å™ªéŸ³/ç¦»ç¾¤ç‚¹") # å¤„ç† -1 çš„æƒ…å†µ
        
        self.results_df = df

    def save_results(self, file_path: str) -> None:
        """
        [Public] å°†ç»“æœä¿å­˜ä¸º Excel æ–‡ä»¶ã€‚
        
        Args:
            file_path (str): è¾“å‡ºè·¯å¾„ï¼Œå¿…é¡»ä»¥ .xlsx ç»“å°¾ã€‚
        """
        if self.results_df is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ run_analysis() æ–¹æ³•ï¼")
            
        self.results_df.to_excel(file_path, index=False)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {file_path}")

    def get_results(self) -> pd.DataFrame:
        """
        [Public] è·å–åˆ†æç»“æœ DataFrameã€‚
        """
        return self.results_df
    
    def get_embeddings(self) -> np.ndarray:
        """
        [Public] è·å–åŸå§‹ SBERT å‘é‡ã€‚
        """
        return self._embeddings

    def save_state(self, file_path: str = "cluster_state.pkl"):
        """
        [Public] ä¿å­˜å½“å‰çš„åˆ†æçŠ¶æ€ï¼ˆæ ¸å¿ƒå‘é‡å’Œç»“æœï¼‰ã€‚
        è¿™æ ·ä¸‹æ¬¡å°±ä¸ç”¨é‡æ–°è·‘ SBERT ç¼–ç äº†ã€‚
        """
        if self._embeddings is None:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ï¼Œè¯·å…ˆè¿è¡Œ run_analysisã€‚")
            return

        state = {
            'embeddings': self._embeddings,           # åŸå§‹ SBERT å‘é‡ (æœ€å®è´µçš„æ•°æ®)
            'umap_embeddings': self._umap_embeddings, # é™ç»´åçš„å‘é‡
            'labels': self._labels,                   # èšç±»æ ‡ç­¾
            'results_df': self.results_df             # æœ€ç»ˆç»“æœè¡¨
        }
        
        with open(file_path, 'wb') as f:
            pickle.dump(state, f)
        print(f"ğŸ’¾ åˆ†æçŠ¶æ€å·²ä¿å­˜è‡³: {file_path} (ä¸‹æ¬¡å¯ç›´æ¥åŠ è½½)")

    def load_state(self, file_path: str = "cluster_state.pkl") -> bool:
        """
        [Public] åŠ è½½ä¹‹å‰çš„åˆ†æçŠ¶æ€ã€‚
        è¿”å› True è¡¨ç¤ºåŠ è½½æˆåŠŸï¼ŒFalse è¡¨ç¤ºæ–‡ä»¶ä¸å­˜åœ¨ã€‚
        """
        if not os.path.exists(file_path):
            print(f"âš ï¸ çŠ¶æ€æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œå°†é‡æ–°è®¡ç®—ã€‚")
            return False
            
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½å†å²çŠ¶æ€: {file_path} ...")
        try:
            with open(file_path, 'rb') as f:
                state = pickle.load(f)
            
            # æ¢å¤å†…éƒ¨å˜é‡
            self._embeddings = state.get('embeddings')
            self._umap_embeddings = state.get('umap_embeddings')
            self._labels = state.get('labels')
            self.results_df = state.get('results_df')
            
            print("âœ… çŠ¶æ€åŠ è½½æˆåŠŸï¼å·²è·³è¿‡ SBERT ç¼–ç æ­¥éª¤ã€‚")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥ï¼Œæ–‡ä»¶å¯èƒ½å·²æŸå: {e}")
            return False

    def re_cluster(self, n_neighbors=15, min_cluster_size=10, keyword_top_n=5):
        """
        [Public] åŸºäºå·²æœ‰çš„å‘é‡ï¼Œé‡æ–°è°ƒæ•´èšç±»å‚æ•° (æé€Ÿç‰ˆ)ã€‚
        å‰æï¼šå¿…é¡»å…ˆ load_state æˆ– run_analysisã€‚
        """
        if self._embeddings is None:
            raise ValueError("æ²¡æœ‰å‘é‡æ•°æ®ï¼è¯·å…ˆåŠ è½½çŠ¶æ€æˆ–è¿è¡Œå®Œæ•´åˆ†æã€‚")
            
        print("ğŸ”„ [Re-Run] åŸºäºç°æœ‰å‘é‡é‡æ–°èšç±»...")
        # åªéœ€è¦é‡è·‘åé¢å‡ æ­¥ï¼Œä¸éœ€è¦é‡è·‘ _encode_text
        self._reduce_dimensions(n_neighbors, 5) # é‡æ–°é™ç»´
        self._perform_clustering(min_cluster_size) # é‡æ–°èšç±»
        
        # ä¸ºäº†ç”ŸæˆæŠ¥å‘Šï¼Œæˆ‘ä»¬éœ€è¦åŸå§‹æ–‡æœ¬ã€‚å¦‚æœ results_df è¿˜åœ¨ï¼Œå¯ä»¥ç›´æ¥å–
        if self.results_df is not None:
            texts = self.results_df['Text'].tolist()
            original = None
            if 'Original_Content' in self.results_df.columns:
                original = self.results_df['Original_Content'].tolist()
            
            self._generate_final_report(texts, keyword_top_n, original)
        else:
            print("âš ï¸ è­¦å‘Šï¼šç¼ºå°‘åŸå§‹æ–‡æœ¬ï¼Œæ— æ³•ç”Ÿæˆå…³é”®è¯æŠ¥å‘Šï¼Œä»…æ›´æ–°äº†èšç±»æ ‡ç­¾ã€‚")

    def merge_similar_clusters(self, threshold: float = 0.9) -> None:
        """
        [Public] åå¤„ç†ï¼šè‡ªåŠ¨åˆå¹¶è¯­ä¹‰æå…¶ç›¸ä¼¼çš„ç°‡ã€‚
        åŸºäºç°‡ä¸­å¿ƒå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå°†ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„ç°‡è¿›è¡Œåˆå¹¶ã€‚
        
        Args:
            threshold (float): åˆå¹¶é˜ˆå€¼ (0.0 - 1.0)ã€‚
                               æ¨è 0.90 - 0.95ã€‚
                               å€¼è¶Šå°ï¼Œåˆå¹¶è¶Šæ¿€è¿›ï¼ˆå‰©ä¸‹çš„ç±»è¶Šå°‘ï¼‰ã€‚
        """
        if self.results_df is None or self._embeddings is None:
            print("âš ï¸ æ— æ³•åˆå¹¶ï¼šç¼ºå°‘åˆ†æç»“æœæˆ–å‘é‡æ•°æ®ã€‚")
            return

        print(f"ğŸ”„ [Post-Process] æ­£åœ¨è¿›è¡Œè¯­ä¹‰åˆå¹¶ (é˜ˆå€¼: {threshold})...")
        
        # 1. å‡†å¤‡æ•°æ®ï¼šåªå¤„ç†æœ‰æ•ˆèšç±» (å¿½ç•¥å™ªéŸ³ -1)
        valid_mask = self.results_df['Cluster'] != -1
        valid_df = self.results_df[valid_mask]
        
        if valid_df.empty:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆèšç±»å¯åˆå¹¶ã€‚")
            return

        # è·å–æ‰€æœ‰å”¯ä¸€çš„èšç±» ID
        cluster_ids = sorted(valid_df['Cluster'].unique())
        
        # 2. è®¡ç®—æ¯ä¸ªç°‡çš„è´¨å¿ƒ (Centroid)
        # è´¨å¿ƒ = è¯¥ç°‡æ‰€æœ‰æ–‡æœ¬å‘é‡çš„å¹³å‡å€¼
        centroids = []
        for cid in cluster_ids:
            # è·å–è¯¥ç°‡å¯¹åº”çš„åŸå§‹å‘é‡ç´¢å¼•
            indices = valid_df[valid_df['Cluster'] == cid].index
            vecs = self._embeddings[indices]
            centroids.append(np.mean(vecs, axis=0))
        
        if len(centroids) < 2:
            print("âš ï¸ èšç±»æ•°é‡ä¸è¶³ 2 ä¸ªï¼Œæ— éœ€åˆå¹¶ã€‚")
            return

        # 3. è®¡ç®—è´¨å¿ƒä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = cosine_similarity(centroids)
        
        # 4. æ‰§è¡Œåˆå¹¶é€»è¾‘ (è´ªå©ªç­–ç•¥)
        # merge_map è®°å½•æ˜ å°„å…³ç³»: {æ—§ID: æ–°ID}
        # åˆå§‹çŠ¶æ€æ¯ä¸ª ID æŒ‡å‘è‡ªå·±
        merge_map = {cid: cid for cid in cluster_ids}
        merged_count = 0
        
        # å€’åºéå†ï¼Œä¼˜å…ˆæŠŠåé¢çš„ç±»åˆå¹¶åˆ°å‰é¢çš„ç±» (ä¿ç•™ ID å°çš„)
        for i in range(len(cluster_ids) - 1, 0, -1):
            source_id = cluster_ids[i]
            
            # å¯»æ‰¾ä¸å½“å‰ç±»æœ€ç›¸ä¼¼çš„å‰åºç±»
            best_match_idx = -1
            best_sim = -1.0
            
            for j in range(i):
                sim = sim_matrix[i][j]
                if sim > threshold and sim > best_sim:
                    best_sim = sim
                    best_match_idx = j
            
            # å¦‚æœæ‰¾åˆ°äº†ç¬¦åˆæ¡ä»¶çš„â€œäº²æˆšâ€
            if best_match_idx != -1:
                target_id = cluster_ids[best_match_idx]
                
                # è¿½æº¯ target_id çš„æœ€ç»ˆå½’å®¿ (é˜²æ­¢é“¾å¼åˆå¹¶ A->B, B->C çš„æƒ…å†µ)
                while target_id != merge_map[target_id]:
                    target_id = merge_map[target_id]
                
                merge_map[source_id] = target_id
                merged_count += 1
                
                # è·å–å…³é”®è¯ç”¨äºæ‰“å°æ—¥å¿—
                src_kw = self.results_df[self.results_df['Cluster'] == source_id]['Keywords'].iloc[0].split(',')[0]
                tgt_kw = self.results_df[self.results_df['Cluster'] == target_id]['Keywords'].iloc[0].split(',')[0]
                print(f"   - åˆå¹¶: C{source_id}[{src_kw}] -> C{target_id}[{tgt_kw}] (ç›¸ä¼¼åº¦: {best_sim:.3f})")

        if merged_count == 0:
            print("âœ… æ²¡æœ‰å‘ç°éœ€è¦åˆå¹¶çš„ç°‡ã€‚")
            return

        # 5. æ›´æ–° DataFrame ä¸­çš„ Cluster åˆ—
        # ä½¿ç”¨ map æ›´æ–°
        new_labels = self.results_df['Cluster'].copy()
        
        # ä»…æ›´æ–° merge_map ä¸­å‘ç”Ÿå˜åŒ–çš„
        for src, tgt in merge_map.items():
            if src != tgt:
                new_labels[new_labels == src] = tgt
        
        self.results_df['Cluster'] = new_labels
        self._labels = new_labels.values # åŒæ­¥æ›´æ–°å†…éƒ¨ labels
        
        # 6. å› ä¸ºåˆå¹¶äº†ç±»ï¼Œå…³é”®è¯å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼ˆå¤§ç±»æ¶µç›–äº†æ›´å¤šå†…å®¹ï¼‰ï¼Œå»ºè®®é‡æ–°æå–å…³é”®è¯
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬æš‚ä¸é‡æ–°æå–æ‰€æœ‰å…³é”®è¯ï¼Œä½†åœ¨å®é™…ä¸šåŠ¡ä¸­å»ºè®®è°ƒç”¨ä¸€æ¬¡ _extract_cluster_keywords
        # å¦‚æœä½ æƒ³è‡ªåŠ¨åˆ·æ–°å…³é”®è¯ï¼Œå–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Šï¼š
        # self._generate_final_report(self.results_df['Text'].tolist(), keyword_top_n=5)
        
        print(f"âœ… åˆå¹¶å®Œæˆï¼å…±å‡å°‘äº† {merged_count} ä¸ªå¾®å‹ç°‡ã€‚")
        print(f"   å½“å‰å‰©ä½™èšç±»æ•°: {len(set(new_labels)) - (1 if -1 in new_labels else 0)}")