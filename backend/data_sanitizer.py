import re
import pandas as pd
from transformers import pipeline
from tqdm import tqdm  # è¿›åº¦æ¡åº“

class TaxDataSanitizer:
    def __init__(self, use_ner=True, device=0, batch_size=64):
        """
        Args:
            use_ner (bool): æ˜¯å¦å¯ç”¨ BERTã€‚å¼€å¯åå°†ä¸å†ä½¿ç”¨â€œå…¬å¸åæ­£åˆ™â€ï¼Œå®Œå…¨ä¾èµ–æ¨¡å‹ã€‚
            device (int): æ˜¾å¡ IDã€‚
            batch_size (int): æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ 64ã€‚
        """
        self.use_ner = use_ner
        self.device = device
        self.batch_size = batch_size
        
        # --- 1. é€šç”¨æ­£åˆ™ (æ— è®ºå¼€ä¸å¼€ NER éƒ½è¦è·‘) ---
        self.regex_date = re.compile(
            r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥|\d{4}å¹´\d{1,2}æœˆ|\d{1,2}æœˆ\d{1,2}æ—¥|'
            r'\d{4}-\d{1,2}-\d{1,2}|\d{4}\.\d{1,2}\.\d{1,2}|20\d{2}å¹´)'
        )
        self.regex_id = re.compile(r'(?<![<a-zA-Z])([a-zA-Z0-9]{10,})(?![>a-zA-Z])')
        self.regex_number = re.compile(r'(?<![<a-zA-Z0-9])(\d+\.?\d*)(?![>a-zA-Z0-9])')

        # --- 2. å¤‡ç”¨æ­£åˆ™ (ä»…å½“ use_ner=False æ—¶æ‰ä½¿ç”¨) ---
        self.regex_company_fallback = re.compile(
            r'([\u4e00-\u9fa5]{2,30}(?:å…¬å¸|åˆ†å…¬å¸|æ”¯è¡Œ|å‚|åˆä½œç¤¾|ç»è¥éƒ¨|å•†è¡Œ|è¶…å¸‚|é…’åº—|é¥­åº—|äº‹åŠ¡?æ‰€))'
        )

        # --- 3. åŠ è½½æ¨¡å‹ ---
        if self.use_ner:
            print(f"ğŸš€ Loading NER model on device cuda:{device}...")
            try:
                # ä¼˜å…ˆä»æœ¬åœ°ç›®å½•åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œé¿å…è¢«å½“ä½œ HuggingFace repo id è§£æ
                from transformers import AutoTokenizer, AutoModelForTokenClassification
                import torch

                local_model_dir = "models/models--shibing624--bert4ner-base-chinese"
                tokenizer = AutoTokenizer.from_pretrained(local_model_dir, use_fast=True)
                model = AutoModelForTokenClassification.from_pretrained(local_model_dir)

                if torch.cuda.is_available():
                    try:
                        model.to(torch.device(f"cuda:{device}"))
                    except Exception:
                        pass

                self.ner_pipeline = pipeline(
                    "token-classification",
                    model=model,
                    tokenizer=tokenizer,
                    aggregation_strategy="simple",
                    device=device
                )
            except Exception as e:
                print(f"âš ï¸ Model load failed, fallback to REGEX ONLY mode: {e}")
                self.use_ner = False

    def _common_preprocess(self, text):
        """é€šç”¨é¢„å¤„ç†ï¼šä»…å¤„ç†æ—¥æœŸå’ŒIDï¼Œç»å¯¹ä¸ç¢°å…¬å¸å"""
        if not isinstance(text, str) or not text.strip():
            return ""
        text = self.regex_date.sub('<æ—¥æœŸ>', text)
        text = self.regex_id.sub('<è¯†åˆ«å·>', text)
        return text

    def _apply_ner_logic(self, text, entities):
        """åº”ç”¨ NER å®ä½“æ›¿æ¢é€»è¾‘"""
        if not entities:
            return text
            
        # ç­›é€‰ ORG å’Œ PER
        valid_ents = [e for e in entities if e['entity_group'] in ['ORG', 'PER']]
        # å€’åºæ›¿æ¢é˜²æ­¢ç´¢å¼•åç§»
        valid_ents.sort(key=lambda x: x['start'], reverse=True)
        
        for ent in valid_ents:
            start, end = ent['start'], ent['end']
            span = text[start:end]
            
            # ä¿æŠ¤æœºåˆ¶ï¼šå¦‚æœ NER æŠ“åˆ°äº†åˆšæ‰æ­£åˆ™ç”Ÿæˆçš„ <æ—¥æœŸ> ç­‰æ ‡ç­¾ï¼Œè·³è¿‡
            if '<' in span and '>' in span:
                continue
                
            replacement = '<ä¼ä¸šå>' if ent['entity_group'] == 'ORG' else '<äººå>'
            text = text[:start] + replacement + text[end:]
        return text

    def process_dataframe(self, df, col_name, output_col=None, progress_callback=None):
        """
        ä¿æŒæ¥å£ä¸å˜ï¼Œå†…éƒ¨æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è·¯å¾„
        
        Args:
            progress_callback (callable): å¯é€‰çš„è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç­¾åä¸º progress_callback(current, total, stage)
                                        ç”¨äºåœ¨ Streamlit æˆ–å…¶ä»–å‰ç«¯æ˜¾ç¤ºè¿›åº¦
        """
        if output_col is None:
            output_col = f"{col_name}_sanitized"

        print(f"âš¡ Processing {len(df)} rows. Logic: {'[NER Model Only]' if self.use_ner else '[Regex Only]'} for Companies.")
        
        if progress_callback:
            progress_callback(0, len(df), "åˆå§‹åŒ–é¢„å¤„ç†...")

        # 1. æå–æ•°æ®å¹¶è¿›è¡Œé€šç”¨é¢„å¤„ç† (Date + ID)
        texts = df[col_name].apply(self._common_preprocess).tolist()

        # 2. åˆ†æ”¯é€»è¾‘
        if self.use_ner:
            # === åˆ†æ”¯ A: ä½¿ç”¨ GPU NER (ä¸è·‘æ­£åˆ™å…¬å¸åŒ¹é…) ===
            final_texts = []
            print(f"Running GPU Batch Inference (Batch Size={self.batch_size})...")
            
            total_batches = (len(texts) + self.batch_size - 1) // self.batch_size
            
            # æ‰¹é‡å¾ªç¯
            for batch_idx, i in enumerate(range(0, len(texts), self.batch_size)):
                batch = texts[i : i + self.batch_size]
                try:
                    # æ˜¾å¼ä¼ é€’ batch_size ä¼˜åŒ–æ¨ç†
                    batch_results = self.ner_pipeline(batch, batch_size=self.batch_size)
                except Exception:
                    batch_results = [[] for _ in batch]

                # æ›¿æ¢å®ä½“
                for text, entities in zip(batch, batch_results):
                    final_texts.append(self._apply_ner_logic(text, entities))
                
                # æ›´æ–°è¿›åº¦å›è°ƒ
                if progress_callback:
                    current_count = min(i + self.batch_size, len(texts))
                    progress_callback(current_count, len(texts), f"NERæ¨ç†ä¸­ ({batch_idx + 1}/{total_batches})...")
            
            texts = final_texts

        else:
            # === åˆ†æ”¯ B: ä»…ä½¿ç”¨æ­£åˆ™ (å›é€€é€»è¾‘) ===
            print("Running Regex Fallback for Companies...")
            # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼åŠ é€Ÿ
            texts = [self.regex_company_fallback.sub('<ä¼ä¸š>', t) for t in texts]
            
            if progress_callback:
                progress_callback(len(texts), len(texts), "æ­£åˆ™è¡¨è¾¾å¼å¤„ç†å®Œæˆ...")

        # 3. é€šç”¨æ”¶å°¾ (æ•°å­—)
        print("Finalizing numbers...")
        texts = [self.regex_number.sub('<æ•°å­—>', t) for t in texts]
        
        if progress_callback:
            progress_callback(len(texts), len(texts), "æ•°å­—æ ‡è®°å®Œæˆ...")

        # 4. å›å¡«
        df[output_col] = texts
        print("âœ… Done.")
        return df

    def sanitize_text(self, text):
        """å•æ¡å¤„ç†æ¥å£ (é€»è¾‘ä¿æŒä¸€è‡´)"""
        text = self._common_preprocess(text)
        
        if self.use_ner:
            # NER è·¯å¾„ï¼šç»ä¸è·‘ regex_company
            try:
                entities = self.ner_pipeline(text)
                text = self._apply_ner_logic(text, entities)
            except:
                pass
        else:
            # æ­£åˆ™è·¯å¾„
            text = self.regex_company_fallback.sub('<ä¼ä¸š>', text)
            
        text = self.regex_number.sub('<æ•°å­—>', text)
        return text