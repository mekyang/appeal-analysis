import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.font_manager as fm

# å°è¯•è®¾ç½®ä¸­æ–‡æ˜¾ç¤º (æ ¹æ®ç³»ç»Ÿä¸åŒå¯èƒ½éœ€è¦è°ƒæ•´)
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class ClusterEvaluator:
    """
    èšç±»æ•ˆæœè¯„ä¼°å·¥å…·ç®± (Cluster Evaluation Toolkit)
    
    ç”¨äºå¯¹ TaxClusteringEngine çš„è¾“å‡ºç»“æœè¿›è¡Œæ•°å­¦æŒ‡æ ‡è®¡ç®—ã€å¯è§†åŒ–å’Œè¯­ä¹‰åˆ†æã€‚
    """

    def __init__(self, df: pd.DataFrame, embeddings: np.ndarray):
        """
        [Public] åˆå§‹åŒ–è¯„ä¼°å™¨ã€‚

        Args:
            df (pd.DataFrame): åŒ…å« 'Text', 'Cluster', 'Keywords' åˆ—çš„ç»“æœè¡¨ã€‚
            embeddings (np.ndarray): å¯¹åº”çš„ SBERT åŸå§‹å‘é‡ (æˆ– UMAP é™ç»´åçš„å‘é‡)ã€‚
                                     å»ºè®®ä¼ å…¥åŸå§‹ SBERT å‘é‡ä»¥è·å¾—æ›´å‡†ç¡®çš„è¯­ä¹‰è·ç¦»ã€‚
        """
        self.df = df
        self.embeddings = embeddings
        
        # é¢„è®¡ç®—ä¸€äº›åŸºç¡€æ©ç 
        self.valid_mask = self.df['Cluster'] != -1
        self.noise_mask = self.df['Cluster'] == -1
        self.n_clusters = len(self.df[self.valid_mask]['Cluster'].unique())

    def compute_metrics(self) -> dict:
        """
        [Public] è®¡ç®—æ ¸å¿ƒæ•°å­¦æŒ‡æ ‡ã€‚
        
        Returns:
            dict: åŒ…å«å™ªéŸ³ç‡ã€è½®å»“ç³»æ•°ç­‰æŒ‡æ ‡çš„å­—å…¸ã€‚
        """
        print("ğŸ“Š [Metric] æ­£åœ¨è®¡ç®—æ•°å­¦æŒ‡æ ‡...")
        
        # 1. å™ªéŸ³æ¯”ä¾‹
        total = len(self.df)
        noise_count = self.noise_mask.sum()
        noise_ratio = noise_count / total
        
        metrics = {
            "Total Samples": total,
            "Valid Clusters": self.n_clusters,
            "Noise Ratio": f"{noise_ratio:.2%}"
        }

        # 2. è½®å»“ç³»æ•° (Silhouette Score)
        # æ³¨æ„ï¼šè½®å»“ç³»æ•°è®¡ç®—é‡å¤§ï¼Œä¸”ä¸èƒ½åŒ…å«å™ªéŸ³ç‚¹ï¼Œè‡³å°‘è¦æœ‰2ä¸ªç°‡
        if self.n_clusters > 1:
            valid_embeddings = self.embeddings[self.valid_mask]
            valid_labels = self.df[self.valid_mask]['Cluster']
            
            # ä½¿ç”¨ä½™å¼¦è·ç¦»è®¡ç®—
            score = silhouette_score(valid_embeddings, valid_labels, metric='cosine')
            metrics['Silhouette Score'] = round(score, 4)
            
            # Calinski-Harabasz Score (æ–¹å·®æ¯”æ ‡å‡†) - åˆ†æ•°è¶Šé«˜è¶Šå¥½
            ch_score = calinski_harabasz_score(valid_embeddings, valid_labels)
            metrics['CH Score'] = round(ch_score, 2)
        else:
            metrics['Silhouette Score'] = "N/A (ç°‡æ•°é‡ä¸è¶³)"

        return metrics

    def plot_size_distribution(self, top_n: int = 20):
        """
        [Public] ç»˜åˆ¶èšç±»å¤§å°åˆ†å¸ƒå›¾ (æŸ±çŠ¶å›¾)ã€‚
        ç”¨äºå‘ç°æ˜¯å¦å­˜åœ¨â€œå·¨å‹ç°‡â€æˆ–â€œé•¿å°¾ç¢ç‰‡â€ã€‚
        """
        print("ğŸ“Š [Plot] æ­£åœ¨ç»˜åˆ¶åˆ†å¸ƒå›¾...")
        plt.figure(figsize=(12, 6))
        
        # ç»Ÿè®¡æ¯ä¸ªç°‡çš„æ•°é‡ï¼ˆä¸å«å™ªéŸ³ï¼‰
        counts = self.df[self.valid_mask]['Cluster'].value_counts().head(top_n)
        
        # è·å–å¯¹åº”çš„å…³é”®è¯ä½œä¸º X è½´æ ‡ç­¾
        cluster_labels = []
        for cid in counts.index:
            kw = self.df[self.df['Cluster'] == cid]['Keywords'].iloc[0]
            # æˆªå–å‰ä¸¤ä¸ªå…³é”®è¯ï¼Œé¿å…å›¾è¡¨å¤ªæŒ¤
            short_kw = ",".join(kw.split(',')[:2]) 
            cluster_labels.append(f"C{cid}\n{short_kw}")

        sns.barplot(x=cluster_labels, y=counts.values, palette="viridis")
        
        plt.title(f"Top {top_n} Largest Clusters Distribution")
        plt.xlabel("Cluster ID & Keywords")
        plt.ylabel("Number of Records")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

    def plot_2d_scatter(self, output_path: str = None):
        """
        [Public] ç»˜åˆ¶ 2D æ•£ç‚¹å›¾å¯è§†åŒ–ã€‚
        
        Args:
            output_path (str): å¦‚æœæä¾›è·¯å¾„ï¼Œå°†ä¿å­˜å›¾ç‰‡ã€‚
        """
        print("ğŸ¨ [Plot] æ­£åœ¨é™ç»´å¹¶ç»˜åˆ¶ 2D æ•£ç‚¹å›¾...")
        
        # ä¸ºäº†ç”»å›¾ï¼Œæˆ‘ä»¬éœ€è¦å°†å‘é‡é™åˆ° 2D
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åœ¨è¯¥ç±»å†…éƒ¨é‡æ–°è·‘ä¸€æ¬¡ UMAP 2Dï¼Œä»…ç”¨äºç”»å›¾ï¼Œä¸å½±å“ä¹‹å‰çš„èšç±»ç»“æœ
        reducer_2d = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine', random_state=42)
        embedding_2d = reducer_2d.fit_transform(self.embeddings)
        
        plt.figure(figsize=(14, 10))
        
        # 1. ç”»å™ªéŸ³ (ç°è‰²)
        if self.noise_mask.any():
            plt.scatter(embedding_2d[self.noise_mask, 0], 
                        embedding_2d[self.noise_mask, 1],
                        c='#E0E0E0', s=5, label='Noise', alpha=0.5)
            
        # 2. ç”»æœ‰æ•ˆèšç±»
        # ä½¿ç”¨ tab20 é¢œè‰²æ¿ï¼ŒåŒºåˆ†åº¦è¾ƒé«˜
        scatter = plt.scatter(embedding_2d[self.valid_mask, 0], 
                              embedding_2d[self.valid_mask, 1],
                              c=self.df[self.valid_mask]['Cluster'], 
                              cmap='tab20', s=8, alpha=0.8)
        
        plt.colorbar(scatter, label='Cluster ID')
        plt.title('Tax Issues 2D Visualization')
        plt.xlabel('UMAP Dim 1')
        plt.ylabel('UMAP Dim 2')
        
        if output_path:
            plt.savefig(output_path, dpi=300)
            print(f"   -> å›¾ç‰‡å·²ä¿å­˜è‡³: {output_path}")
        plt.show()

    def analyze_similarity(self):
        """
        [Public] è®¡ç®—ç°‡ä¸­å¿ƒç›¸ä¼¼åº¦çƒ­åŠ›å›¾ã€‚
        å¸®åŠ©å‘ç°ï¼šæ˜¯å¦æœ‰ä¸¤ä¸ªç°‡å…¶å®æ˜¯åœ¨è¯´åŒä¸€ä»¶äº‹ï¼ˆåº”è¯¥åˆå¹¶ï¼‰ï¼Ÿ
        """
        if self.n_clusters < 2:
            print("âŒ ç°‡æ•°é‡ä¸è¶³ï¼Œæ— æ³•åˆ†æç›¸ä¼¼åº¦ã€‚")
            return

        print("ğŸ” [Analysis] æ­£åœ¨åˆ†æç°‡é—´è¯­ä¹‰é‡å åº¦...")
        
        # 1. è®¡ç®—æ¯ä¸ªç°‡çš„â€œè´¨å¿ƒâ€ (Centroid) - å³è¯¥ç°‡æ‰€æœ‰å‘é‡çš„å¹³å‡å€¼
        cluster_ids = sorted(self.df[self.valid_mask]['Cluster'].unique())
        centroids = []
        labels = []
        
        for cid in cluster_ids:
            # è·å–è¯¥ç°‡çš„æ‰€æœ‰å‘é‡
            indices = self.df[self.df['Cluster'] == cid].index
            cluster_vecs = self.embeddings[indices]
            centroid = np.mean(cluster_vecs, axis=0)
            centroids.append(centroid)
            
            # è·å–æ ‡ç­¾ç”¨äºç”»å›¾
            kw = self.df[self.df['Cluster'] == cid]['Keywords'].iloc[0]
            labels.append(f"C{cid}: {kw.split(',')[0]}") # åªå–ç¬¬ä¸€ä¸ªå…³é”®è¯

        # 2. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = cosine_similarity(centroids)
        
        # 3. ç»˜åˆ¶çƒ­åŠ›å›¾
        plt.figure(figsize=(12, 10))
        sns.heatmap(sim_matrix, xticklabels=labels, yticklabels=labels, 
                    cmap="RdBu_r", center=0.5, annot=False)
        plt.title("Cluster Semantic Similarity Matrix (1.0 = Highly Similar)")
        plt.xticks(rotation=90)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.show()
        
        # 4. è‡ªåŠ¨ç»™å‡ºå»ºè®®
        # æ‰¾å‡ºç›¸ä¼¼åº¦å¤§äº 0.85 çš„éå¯¹è§’çº¿å…ƒç´ 
        print("\n--- âš ï¸ åˆå¹¶å»ºè®® (Similarity > 0.85) ---")
        found = False
        for i in range(len(cluster_ids)):
            for j in range(i + 1, len(cluster_ids)):
                if sim_matrix[i][j] > 0.85:
                    print(f"å»ºè®®æ£€æŸ¥: [{labels[i]}] <==> [{labels[j]}] (ç›¸ä¼¼åº¦: {sim_matrix[i][j]:.3f})")
                    found = True
        if not found:
            print("æœªå‘ç°æ˜æ˜¾çš„é‡å ç°‡ï¼Œèšç±»åŒºåˆ†åº¦è‰¯å¥½ã€‚")

    def run_full_report(self):
        """
        [Public] ä¸€é”®è¿è¡Œæ‰€æœ‰ä½“æ£€é¡¹ç›®
        """
        print("="*30)
        print("  CLUSTER EVALUATION REPORT  ")
        print("="*30)
        
        # 1. æŒ‡æ ‡
        metrics = self.compute_metrics()
        for k, v in metrics.items():
            print(f"{k}: {v}")
        print("-" * 30)
        
        # 2. åˆ†å¸ƒ
        self.plot_size_distribution()
        
        # 3. æ•£ç‚¹å›¾
        self.plot_2d_scatter()
        
        # 4. ç›¸ä¼¼åº¦
        self.analyze_similarity()